{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Spark session is being created\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SpamDetection Notebook\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|spam|             message|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "+----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Reading the file from the hdfs wherein the file has the data not comma separated but the tab separated\n",
    "raw = spark.read.option(\"delimiter\",\"\\t\").csv(\"/user/edureka_960126/SMSSpamCollection\").toDF(\"spam\",\"message\")\n",
    "raw.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\n",
      "|spam|             message|               words|\n",
      "+----+--------------------+--------------------+\n",
      "| ham|Go until jurong p...|[go, until, juron...|\n",
      "+----+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract word\n",
    "#Tokenization is the process of taking text (such as a sentence) and breaking it into individual terms (usually words). \n",
    "#A simple Tokenizer class provides this functionality.\n",
    "#Note that all resulting words are the small letter words\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "#Here the tokeniser take the input column and the output column\n",
    "tokenizer = Tokenizer().setInputCol(\"message\").setOutputCol(\"words\")\n",
    "\n",
    "#Transform the Data so that new column is created over the raw dataframe\n",
    "transformed = tokenizer.transform(raw)\n",
    "transformed.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+\n",
      "|spam|             message|               words|            filtered|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "| ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "#Stop words are words which should be excluded from the input, typically because the words appear frequently \n",
    "#and don’t carry as much meaning\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "remover = StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "cleaned = remover.transform(transformed)\n",
    "cleaned.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+\n",
      "|spam|             message|               words|            filtered|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "| ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# custom stopwords\n",
    "#so here we are in custom manner removing the stop words whrein we have added the - as a stop word\n",
    "stopwords = StopWordsRemover().getStopWords() + [\"-\"]\n",
    "remover = StopWordsRemover().setStopWords(stopwords).setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "cleaned = remover.transform(transformed)\n",
    "cleaned.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      spam                                            message  \\\n",
      "0      ham  Go until jurong point, crazy.. Available only ...   \n",
      "1      ham                      Ok lar... Joking wif u oni...   \n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3      ham  U dun say so early hor... U c already then say...   \n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
      "5     spam  FreeMsg Hey there darling it's been 3 week's n...   \n",
      "6      ham  Even my brother is not like to speak with me. ...   \n",
      "7      ham  As per your request 'Melle Melle (Oru Minnamin...   \n",
      "8     spam  WINNER!! As a valued network customer you have...   \n",
      "9     spam  Had your mobile 11 months or more? U R entitle...   \n",
      "10     ham  I'm gonna be home soon and i don't want to tal...   \n",
      "11    spam  SIX chances to win CASH! From 100 to 20,000 po...   \n",
      "12    spam  URGENT! You have won a 1 week FREE membership ...   \n",
      "13     ham  I've been searching for the right words to tha...   \n",
      "14     ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
      "15    spam  XXXMobileMovieClub: To use your credit, click ...   \n",
      "16     ham                         Oh k...i'm watching here:)   \n",
      "17     ham  Eh u remember how 2 spell his name... Yes i di...   \n",
      "18     ham  Fine if thats the way u feel. Thats the way ...   \n",
      "19    spam  England v Macedonia - dont miss the goals/team...   \n",
      "20     ham          Is that seriously how you spell his name?   \n",
      "21     ham    I‘m going to try for 2 months ha ha only joking   \n",
      "22     ham  So ü pay first lar... Then when is da stock co...   \n",
      "23     ham  Aft i finish my lunch then i go str down lor. ...   \n",
      "24     ham  Ffffffffff. Alright no way I can meet up with ...   \n",
      "25     ham  Just forced myself to eat a slice. I'm really ...   \n",
      "26     ham                     Lol your always so convincing.   \n",
      "27     ham  Did you catch the bus ? Are you frying an egg ...   \n",
      "28     ham  I'm back &amp; we're packing the car now, I'll...   \n",
      "29     ham  Ahhh. Work. I vaguely remember that! What does...   \n",
      "...    ...                                                ...   \n",
      "5544   ham           Armand says get your ass over to epsilon   \n",
      "5545   ham             U still havent got urself a jacket ah?   \n",
      "5546   ham  I'm taking derek &amp; taylor to walmart, if I...   \n",
      "5547   ham      Hi its in durban are you still on this number   \n",
      "5548   ham         Ic. There are a lotta childporn cars then.   \n",
      "5549  spam  Had your contract mobile 11 Mnths? Latest Moto...   \n",
      "5550   ham                 No, I was trying it all weekend ;V   \n",
      "5551   ham  You know, wot people wear. T shirts, jumpers, ...   \n",
      "5552   ham        Cool, what time you think you can get here?   \n",
      "5553   ham  Wen did you get so spiritual and deep. That's ...   \n",
      "5554   ham  Have a safe trip to Nigeria. Wish you happines...   \n",
      "5555   ham                        Hahaha..use your brain dear   \n",
      "5556   ham  Well keep in mind I've only got enough gas for...   \n",
      "5557   ham  Yeh. Indians was nice. Tho it did kane me off ...   \n",
      "5558   ham  Yes i have. So that's why u texted. Pshew...mi...   \n",
      "5559   ham  No. I meant the calculation is the same. That ...   \n",
      "5560   ham                             Sorry, I'll call later   \n",
      "5561   ham  if you aren't here in the next  &lt;#&gt;  hou...   \n",
      "5562   ham                  Anything lor. Juz both of us lor.   \n",
      "5563   ham  Get me out of this dump heap. My mom decided t...   \n",
      "5564   ham  Ok lor... Sony ericsson salesman... I ask shuh...   \n",
      "5565   ham                                Ard 6 like dat lor.   \n",
      "5566   ham  Why don't you wait 'til at least wednesday to ...   \n",
      "5567   ham                                       Huh y lei...   \n",
      "5568  spam  REMINDER FROM O2: To get 2.50 pounds free call...   \n",
      "5569  spam  This is the 2nd time we have tried 2 contact u...   \n",
      "5570   ham               Will ü b going to esplanade fr home?   \n",
      "5571   ham  Pity, * was in mood for that. So...any other s...   \n",
      "5572   ham  The guy did some bitching but I acted like i'd...   \n",
      "5573   ham                         Rofl. Its true to its name   \n",
      "\n",
      "                                                  words  \\\n",
      "0     [go, until, jurong, point,, crazy.., available...   \n",
      "1                  [ok, lar..., joking, wif, u, oni...]   \n",
      "2     [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
      "3     [u, dun, say, so, early, hor..., u, c, already...   \n",
      "4     [nah, i, don't, think, he, goes, to, usf,, he,...   \n",
      "5     [freemsg, hey, there, darling, it's, been, 3, ...   \n",
      "6     [even, my, brother, is, not, like, to, speak, ...   \n",
      "7     [as, per, your, request, 'melle, melle, (oru, ...   \n",
      "8     [winner!!, as, a, valued, network, customer, y...   \n",
      "9     [had, your, mobile, 11, months, or, more?, u, ...   \n",
      "10    [i'm, gonna, be, home, soon, and, i, don't, wa...   \n",
      "11    [six, chances, to, win, cash!, from, 100, to, ...   \n",
      "12    [urgent!, you, have, won, a, 1, week, free, me...   \n",
      "13    [i've, been, searching, for, the, right, words...   \n",
      "14         [i, have, a, date, on, sunday, with, will!!]   \n",
      "15    [xxxmobilemovieclub:, to, use, your, credit,, ...   \n",
      "16                      [oh, k...i'm, watching, here:)]   \n",
      "17    [eh, u, remember, how, 2, spell, his, name...,...   \n",
      "18    [fine, if, thats, the, way, u, feel., thats,...   \n",
      "19    [england, v, macedonia, -, dont, miss, the, go...   \n",
      "20    [is, that, seriously, how, you, spell, his, na...   \n",
      "21    [i‘m, going, to, try, for, 2, months, ha, ha, ...   \n",
      "22    [so, ü, pay, first, lar..., then, when, is, da...   \n",
      "23    [aft, i, finish, my, lunch, then, i, go, str, ...   \n",
      "24    [ffffffffff., alright, no, way, i, can, meet, ...   \n",
      "25    [just, forced, myself, to, eat, a, slice., i'm...   \n",
      "26                 [lol, your, always, so, convincing.]   \n",
      "27    [did, you, catch, the, bus, ?, are, you, fryin...   \n",
      "28    [i'm, back, &amp;, we're, packing, the, car, n...   \n",
      "29    [ahhh., work., i, vaguely, remember, that!, wh...   \n",
      "...                                                 ...   \n",
      "5544  [armand, says, get, your, ass, over, to, epsilon]   \n",
      "5545    [u, still, havent, got, urself, a, jacket, ah?]   \n",
      "5546  [i'm, taking, derek, &amp;, taylor, to, walmar...   \n",
      "5547  [hi, its, in, durban, are, you, still, on, thi...   \n",
      "5548  [ic., there, are, a, lotta, childporn, cars, t...   \n",
      "5549  [had, your, contract, mobile, 11, mnths?, late...   \n",
      "5550        [no,, i, was, trying, it, all, weekend, ;v]   \n",
      "5551  [you, know,, wot, people, wear., t, shirts,, j...   \n",
      "5552  [cool,, what, time, you, think, you, can, get,...   \n",
      "5553  [wen, did, you, get, so, spiritual, and, deep....   \n",
      "5554  [have, a, safe, trip, to, nigeria., wish, you,...   \n",
      "5555                   [hahaha..use, your, brain, dear]   \n",
      "5556  [well, keep, in, mind, i've, only, got, enough...   \n",
      "5557  [yeh., indians, was, nice., tho, it, did, kane...   \n",
      "5558  [yes, i, have., so, that's, why, u, texted., p...   \n",
      "5559  [no., i, meant, the, calculation, is, the, sam...   \n",
      "5560                        [sorry,, i'll, call, later]   \n",
      "5561  [if, you, aren't, here, in, the, next, , &lt;#...   \n",
      "5562          [anything, lor., juz, both, of, us, lor.]   \n",
      "5563  [get, me, out, of, this, dump, heap., my, mom,...   \n",
      "5564  [ok, lor..., sony, ericsson, salesman..., i, a...   \n",
      "5565                          [ard, 6, like, dat, lor.]   \n",
      "5566  [why, don't, you, wait, 'til, at, least, wedne...   \n",
      "5567                                   [huh, y, lei...]   \n",
      "5568  [reminder, from, o2:, to, get, 2.50, pounds, f...   \n",
      "5569  [this, is, the, 2nd, time, we, have, tried, 2,...   \n",
      "5570      [will, ü, b, going, to, esplanade, fr, home?]   \n",
      "5571  [pity,, *, was, in, mood, for, that., so...any...   \n",
      "5572  [the, guy, did, some, bitching, but, i, acted,...   \n",
      "5573                  [rofl., its, true, to, its, name]   \n",
      "\n",
      "                                               filtered  \\\n",
      "0     [go, jurong, point,, crazy.., available, bugis...   \n",
      "1                  [ok, lar..., joking, wif, u, oni...]   \n",
      "2     [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
      "3     [u, dun, say, early, hor..., u, c, already, sa...   \n",
      "4     [nah, don't, think, goes, usf,, lives, around,...   \n",
      "5     [freemsg, hey, darling, it's, 3, week's, word,...   \n",
      "6     [even, brother, like, speak, me., treat, like,...   \n",
      "7     [per, request, 'melle, melle, (oru, minnaminun...   \n",
      "8     [winner!!, valued, network, customer, selected...   \n",
      "9     [mobile, 11, months, more?, u, r, entitled, up...   \n",
      "10    [i'm, gonna, home, soon, don't, want, talk, st...   \n",
      "11    [six, chances, win, cash!, 100, 20,000, pounds...   \n",
      "12    [urgent!, 1, week, free, membership, £100,000,...   \n",
      "13    [i've, searching, right, words, thank, breathe...   \n",
      "14                               [date, sunday, will!!]   \n",
      "15    [xxxmobilemovieclub:, use, credit,, click, wap...   \n",
      "16                      [oh, k...i'm, watching, here:)]   \n",
      "17    [eh, u, remember, 2, spell, name..., yes, did....   \n",
      "18    [fine, thats, way, u, feel., thats, way, got...   \n",
      "19    [england, v, macedonia, dont, miss, goals/team...   \n",
      "20                            [seriously, spell, name?]   \n",
      "21         [i‘m, going, try, 2, months, ha, ha, joking]   \n",
      "22         [ü, pay, first, lar..., da, stock, comin...]   \n",
      "23    [aft, finish, lunch, go, str, lor., ard, 3, sm...   \n",
      "24           [ffffffffff., alright, way, meet, sooner?]   \n",
      "25    [forced, eat, slice., i'm, really, hungry, tho...   \n",
      "26                           [lol, always, convincing.]   \n",
      "27    [catch, bus, ?, frying, egg, ?, make, tea?, ea...   \n",
      "28    [i'm, back, &amp;, we're, packing, car, now,, ...   \n",
      "29    [ahhh., work., vaguely, remember, that!, feel,...   \n",
      "...                                                 ...   \n",
      "5544                  [armand, says, get, ass, epsilon]   \n",
      "5545       [u, still, havent, got, urself, jacket, ah?]   \n",
      "5546  [i'm, taking, derek, &amp;, taylor, walmart,, ...   \n",
      "5547                        [hi, durban, still, number]   \n",
      "5548               [ic., lotta, childporn, cars, then.]   \n",
      "5549  [contract, mobile, 11, mnths?, latest, motorol...   \n",
      "5550                         [no,, trying, weekend, ;v]   \n",
      "5551  [know,, wot, people, wear., shirts,, jumpers,,...   \n",
      "5552                   [cool,, time, think, get, here?]   \n",
      "5553        [wen, get, spiritual, deep., that's, great]   \n",
      "5554  [safe, trip, nigeria., wish, happiness, soon, ...   \n",
      "5555                         [hahaha..use, brain, dear]   \n",
      "5556  [well, keep, mind, i've, got, enough, gas, one...   \n",
      "5557  [yeh., indians, nice., tho, kane, bit, he., sh...   \n",
      "5558  [yes, have., that's, u, texted., pshew...missi...   \n",
      "5559  [no., meant, calculation, same., , &lt;#&gt;, ...   \n",
      "5560                        [sorry,, i'll, call, later]   \n",
      "5561  [aren't, next, , &lt;#&gt;, , hours, imma, fli...   \n",
      "5562                    [anything, lor., juz, us, lor.]   \n",
      "5563  [get, dump, heap., mom, decided, come, lowes.,...   \n",
      "5564  [ok, lor..., sony, ericsson, salesman..., ask,...   \n",
      "5565                          [ard, 6, like, dat, lor.]   \n",
      "5566  [don't, wait, 'til, least, wednesday, see, get...   \n",
      "5567                                      [huh, lei...]   \n",
      "5568  [reminder, o2:, get, 2.50, pounds, free, call,...   \n",
      "5569  [2nd, time, tried, 2, contact, u., u, £750, po...   \n",
      "5570                [ü, b, going, esplanade, fr, home?]   \n",
      "5571    [pity,, *, mood, that., so...any, suggestions?]   \n",
      "5572  [guy, bitching, acted, like, i'd, interested, ...   \n",
      "5573                                [rofl., true, name]   \n",
      "\n",
      "                                               features  \n",
      "0     (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "1     (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "2     (0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "3     (2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "4     (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5     (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "6     (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "7     (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "8     (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "9     (1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "10    (0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
      "11    (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "12    (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "13    (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "14    (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "15    (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "16    (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "17    (1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "18    (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "19    (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "20    (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "21    (0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "22    (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "23    (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "24    (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "25    (0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, ...  \n",
      "26    (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "27    (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "28    (0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
      "29    (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "...                                                 ...  \n",
      "5544  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "5545  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5546  (0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, ...  \n",
      "5547  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5548  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5549  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5550  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5551  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5552  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "5553  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "5554  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5555  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5556  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5557  (0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, ...  \n",
      "5558  (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5559  (0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, ...  \n",
      "5560  (0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5561  (0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
      "5562  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5563  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "5564  (0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5565  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5566  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "5567  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5568  (0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "5569  (1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5570  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5571  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5572  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "5573  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "\n",
      "[5574 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Generate features\n",
    "#CountVectorizer and CountVectorizerModel aim to help convert a collection of text documents to vectors of token counts.\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n",
    "cvmodel = CountVectorizer().setInputCol(\"filtered\").setOutputCol(\"features\").fit(cleaned)\n",
    "featured = cvmodel.transform(cleaned)\n",
    "print(featured.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to binary label\n",
    "#Here we are just doing the indexing that is actually converting the spam column into 0 and 1 which are labels for ham and spam\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "indexer = StringIndexer().setInputCol(\"spam\").setOutputCol(\"label\").fit(featured)\n",
    "indexed = indexer.transform(featured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|spam|             message|               words|            filtered|            features|label|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| ham| &lt;#&gt;  in mc...|[, &lt;#&gt;, , i...|[, &lt;#&gt;, , m...|(13457,[3,7,5193,...|  0.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split to train and test\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "training, test = indexed.randomSplit([0.7, 0.3], seed = 12345)\n",
    "training.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(13457,[3,12,168,...|  0.0|       0.0|\n",
      "|(13457,[3,13,80,8...|  0.0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "('Accuracy', 0.5)\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "#so here Logistic Regression is used for predicting whether it is a spam or not\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "lrModel = lr.fit(training)\n",
    "predictions = lrModel.transform(test)\n",
    "predictions.select(\"features\", \"label\", \"prediction\").show(2)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy', 0.5046082949308756)\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "#so here Random Forest Regression is used for predicting whether it is a spam or not\n",
    "from pyspark.ml.classification import RandomForestClassificationModel, RandomForestClassifier\n",
    "rf = RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setNumTrees(10)\n",
    "model = rf.fit(training)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ngrams                                                                                                                                                                                       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[go jurong, jurong point,, point, crazy.., crazy.. available, available bugis, bugis n, n great, great world, world la, la e, e buffet..., buffet... cine, cine got, got amore, amore wat...]|\n",
      "|[ok lar..., lar... joking, joking wif, wif u, u oni...]                                                                                                                                      |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "ngram = NGram().setN(2).setInputCol(\"filtered\").setOutputCol(\"ngrams\")\n",
    "ngramDataFrame = ngram.transform(cleaned)\n",
    "ngramDataFrame.select(\"ngrams\").show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|spam|             message|               words|            filtered|              ngrams|            features|label|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| ham| &lt;#&gt;  in mc...|[, &lt;#&gt;, , i...|[, &lt;#&gt;, , m...|[ &lt;#&gt;, &lt;...|(37490,[0,1,11013...|  0.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(37490,[6607,7274...|  0.0|       0.0|\n",
      "|(37490,[10,6515,6...|  0.0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "('Accuracy', 0.5)\n"
     ]
    }
   ],
   "source": [
    "#We have to use the ngram colums instead of filtered to check whether the accuracy is improved or not\n",
    "#Feature detection using the ngrams column\n",
    "cv_ngram_model = CountVectorizer().setInputCol(\"ngrams\").setOutputCol(\"features\").fit(ngramDataFrame)\n",
    "featured = cv_ngram_model.transform(ngramDataFrame)\n",
    "\n",
    "#Indexing of the strings in the column\n",
    "indexer = StringIndexer().setInputCol(\"spam\").setOutputCol(\"label\").fit(featured)\n",
    "indexed = indexer.transform(featured)\n",
    "\n",
    "#Splitting of the Data between the training and the test Data\n",
    "training, test = indexed.randomSplit([0.7, 0.3], seed = 12345)\n",
    "training.show(1)\n",
    "\n",
    "# Logistic regression\n",
    "#so here Logistic Regression is used for predicting whether it is a spam or not\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "lrModel = lr.fit(training)\n",
    "predictions = lrModel.transform(test)\n",
    "predictions.select(\"features\", \"label\", \"prediction\").show(2)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Accuracy\", accuracy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ngrams                                                                                                                                                                                                                                                           |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[go jurong point,, jurong point, crazy.., point, crazy.. available, crazy.. available bugis, available bugis n, bugis n great, n great world, great world la, world la e, la e buffet..., e buffet... cine, buffet... cine got, cine got amore, got amore wat...]|\n",
      "|[ok lar... joking, lar... joking wif, joking wif u, wif u oni...]                                                                                                                                                                                                |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|spam|             message|               words|            filtered|              ngrams|            features|label|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| ham| &lt;#&gt;  in mc...|[, &lt;#&gt;, , i...|[, &lt;#&gt;, , m...|[ &lt;#&gt; , &lt...|(37412,[0,9359,16...|  0.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(37412,[6615,1048...|  0.0|       0.0|\n",
      "|(37412,[4926,7629...|  0.0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "('Accuracy', 0.5)\n"
     ]
    }
   ],
   "source": [
    "#Tri-gram algorithm working\n",
    "\n",
    "from pyspark.ml.feature import NGram\n",
    "ngram = NGram().setN(3).setInputCol(\"filtered\").setOutputCol(\"ngrams\")\n",
    "ngramDataFrame = ngram.transform(cleaned)\n",
    "ngramDataFrame.select(\"ngrams\").show(2, False)\n",
    "\n",
    "#We have to use the ngram colums instead of filtered to check whether the accuracy is improved or not\n",
    "#Feature detection using the ngrams column\n",
    "cv_ngram_model = CountVectorizer().setInputCol(\"ngrams\").setOutputCol(\"features\").fit(ngramDataFrame)\n",
    "featured = cv_ngram_model.transform(ngramDataFrame)\n",
    "\n",
    "#Indexing of the strings in the column\n",
    "indexer = StringIndexer().setInputCol(\"spam\").setOutputCol(\"label\").fit(featured)\n",
    "indexed = indexer.transform(featured)\n",
    "\n",
    "#Splitting of the Data between the training and the test Data\n",
    "training, test = indexed.randomSplit([0.7, 0.3], seed = 12345)\n",
    "training.show(1)\n",
    "\n",
    "# Logistic regression\n",
    "#so here Logistic Regression is used for predicting whether it is a spam or not\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "lrModel = lr.fit(training)\n",
    "predictions = lrModel.transform(test)\n",
    "predictions.select(\"features\", \"label\", \"prediction\").show(2)\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Accuracy\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "tokenizer = Tokenizer().setInputCol(\"message\").setOutputCol(\"words\")\n",
    "\n",
    "stopwords = StopWordsRemover().getStopWords()+ [\"-\"]\n",
    "remover = StopWordsRemover().setStopWords(stopwords).setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "cvmodel = CountVectorizer().setInputCol(\"filtered\").setOutputCol(\"features\")\n",
    "indexer = StringIndexer().setInputCol(\"spam\").setOutputCol(\"label\")\n",
    "lr = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "pipeline = Pipeline().setStages([tokenizer, remover, cvmodel, indexer, lr])\n",
    "model = pipeline.fit(raw)\n",
    "model.write().overwrite().save(\"/user/edureka_960126/spam_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_4c8bb940fdc2767d6890"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = PipelineModel.load(\"/user/edureka_960126/spam_model\")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing of the stored ML model on the HDFS\n",
    "\n",
    "#predictions =pipeline.transform(raw)\n",
    "#predictions.select(\"label\",\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:14:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:14:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:14:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:15:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:15:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:15:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:15:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:16:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:16:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:16:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:16:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:17:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:17:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:17:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:17:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:18:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:18:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:18:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:18:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:19:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:19:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:19:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:19:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:20:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:20:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:20:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:20:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:21:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:21:15\n",
      "-------------------------------------------\n",
      "1\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  1.0|       0.0|\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:21:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:21:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:22:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:22:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:22:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:22:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:23:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:23:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:23:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:23:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:24:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:24:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:24:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:24:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:25:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:25:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:25:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:25:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:26:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:26:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:26:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:26:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:27:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:27:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:27:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:27:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:28:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:28:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:28:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:28:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:29:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:29:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:29:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:29:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:30:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:30:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:30:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:30:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:31:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:31:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:31:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:31:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:32:00\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:32:15\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:32:30\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-21 04:32:45\n",
      "-------------------------------------------\n",
      "\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pipeline Model has already been created will be used to work on the the streaming data sent to the flume\n",
    "#streaming data will be retreived by the Spark Streaming to predict the result\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pyspark.streaming.flume import FlumeUtils\n",
    "\n",
    "import json\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "#Model stored in the hdfs to be used for the machine learning application\n",
    "#is saved as variable pipeline\n",
    "\n",
    "\n",
    "#Schema is being set for the dataframe creation out of the records\n",
    "cSchema = StructType([StructField(\"spam\", StringType(),nullable=True),StructField(\"message\", StringType(),nullable=True)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Process for working out the prediction\n",
    "def process(rdd):\n",
    "    \n",
    "        # Get the singleton instance of SparkSession\n",
    "        #spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "\n",
    "        #Conversion to the Data Frame to work on the dataframe for working out the predictions\n",
    "        df = spark.createDataFrame(rdd,schema=cSchema)\n",
    "\n",
    "        #Now we work on the dataframe for producing the predictions\n",
    "        cols = df.columns\n",
    "\n",
    "        #Getting the output after passing the data frame through vector assembler to produce the dataframe with vectorised features\n",
    "        df = pipeline.transform(df)\n",
    "        \n",
    "        df.select(\"label\",\"prediction\").show()\n",
    "        \n",
    "\n",
    "#starting of the Spark streaming Context\n",
    "ssc= StreamingContext(spark.sparkContext, 15)\n",
    "\n",
    "#Flume stream is generated after the spark streaming receiver is connected to custom spark streaming sink created at the host with a given port\n",
    "flumeStream = FlumeUtils.createPollingStream(ssc, [('ip-20-0-41-164.ec2.internal' , 9090)])\n",
    "\n",
    "#Flume_microbatches count\n",
    "flumeStream.count().pprint()\n",
    "\n",
    "\n",
    "#RDD[Strings]\n",
    "\n",
    "#Here we get the strings of the json format input data \n",
    "lines = flumeStream.map(lambda x: x[1])\n",
    "\n",
    "#RDD of Dicts or JSON objects by extracting the json objects from the string \n",
    "records_dict=lines.map(lambda x: json.loads(x))\n",
    "\n",
    "#Rows RDD rows rdd is created here\n",
    "rows_rdd=records_dict.map(lambda res: Row(res['spam'],res['message']))\n",
    "\n",
    "rows_rdd.foreachRDD(process)\n",
    "\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
