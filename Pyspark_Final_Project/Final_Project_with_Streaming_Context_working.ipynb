{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import countDistinct, avg\n",
    "from pyspark.sql.functions import dayofmonth,dayofyear,year,month,hour,weekofyear,date_format\n",
    "from pyspark.sql.functions import col as func_col\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "###Description of the Data\n",
    "\n",
    "##datetime - hourly date + timestamp\n",
    "\n",
    "##season -  1 = spring, 2 = summer, 3 = fall, 4 = winter \n",
    "\n",
    "##holiday - whether the day is considered a holiday\n",
    "\n",
    "##workingday - whether the day is neither a weekend nor holiday\n",
    "\n",
    "##weather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "\n",
    "##2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "\n",
    "##3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "\n",
    "##4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "\n",
    "#temp - temperature in Celsius\n",
    "\n",
    "#atemp - \"feels like\" temperature in Celsius\n",
    "\n",
    "#humidity - relative humidity\n",
    "\n",
    "#windspeed - wind speed\n",
    "\n",
    "#casual - number of non-registered user rentals initiated\n",
    "#registered - number of registered user rentals initiated\n",
    "#count - number of total rentals\n",
    "\n",
    "\n",
    "\n",
    "####-----------Created a ML Model using the Season Categorical Variable and other Continuous Variables of temp , humidity , windspeed -----------------\n",
    "####-----------Reason in the problem statement exploding of season column was asked , but in same manner one hot encoding can be performed of weather categorical variable also "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "for part in cwd.split('/'):\n",
    "    if part.lower().startswith('edureka'):\n",
    "        user_id = part.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Edureka_960126 : Spark SQL'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_name = '{0} : Spark SQL'.format(user_id)\n",
    "app_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration of the Spark Session\n",
    "conf = SparkConf()  # create the configuration\n",
    "conf.set('spark.driver.extraClassPath', \"/usr/share/cmf/common_jars/mysql-connector-java-5.1.15.jar\")  # set the spark.jars\n",
    "conf.set('spark.executor.extraClassPath', \"/usr/share/cmf/common_jars/mysql-connector-java-5.1.15.jar\")\n",
    "\n",
    "#Spark Session object\n",
    "spark = SparkSession.builder.config(conf=conf).appName(app_name).getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to get the HDFS file path \n",
    "def get_hdfs_filepath(file_name):\n",
    "    my_hdfs = '/user/{0}'.format(user_id.lower())\n",
    "    return os.path.join(my_hdfs, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##loading of the training Data and then analysing the structure of Data using various commands \n",
    "\n",
    "#Training csv from the hdfs path\n",
    "TRAIN_CSV = get_hdfs_filepath('train.csv')\n",
    "\n",
    "# Reading of the Data Set\n",
    "train_df=spark.read.csv(TRAIN_CSV,inferSchema=True,header=True)\n",
    "\n",
    "#Sample showing of the Data\n",
    "train_df.show()\n",
    "\n",
    "#Get summary of data and variable types and other summaries\n",
    "train_df.printSchema()\n",
    "\n",
    "train_df.describe()\n",
    "\n",
    "#summaries of Data\n",
    "train_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Decide which columns should be categorical and then convert them accordingly \n",
    "#Check for any missing value in dataset and treat it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the timestamp into various attributes - as per the problem statement\n",
    "\n",
    "train_df=train_df.withColumn('day',dayofmonth(train_df[\"datetime\"]))\n",
    "train_df=train_df.withColumn('month',month(train_df[\"datetime\"]))\n",
    "train_df=train_df.withColumn('year',year(train_df[\"datetime\"]))\n",
    "train_df=train_df.withColumn('hour',hour(train_df[\"datetime\"]))\n",
    "train_df=train_df.drop(\"datetime\")\n",
    "\n",
    "#showing the data after changing the datetime column\n",
    "train_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore how count varies with different features such as hour, month, etc\n",
    "train_df.groupby(\"hour\").count().orderBy(\"hour\").show()\n",
    "train_df.groupby(\"month\").count().orderBy(\"month\").show()\n",
    "train_df.groupby(\"year\").count().orderBy(\"year\").show()\n",
    "train_df.groupby(\"day\").count().orderBy(\"day\").show()\n",
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explode season column into separate columns such as season_<val> and drop season , the reason it is being done is\n",
    "#Called a technique \" Dummy encoding \" to be used in the Linear regression model\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "######Note that the below is working code to convert the season into season_val but I have not used it because\n",
    "####### I have used one hot encoding for getting the codes out of the categorical variables \n",
    "####### Dummy encoding and dropping one column so season with 1 = 0 0 0 and with 2 = 0 0 1 and so on\n",
    "\n",
    "#So here we selected the season column and then collect the distinct values out of the season column \n",
    "\n",
    "#season_categ = train_df.select('season').distinct().rdd.flatMap(lambda x:x).collect()\n",
    "\n",
    "#Now we are using the SQL functions \n",
    "\n",
    "#exprs = [F.when(F.col('season') == cat,1).otherwise(0).alias('season'+str(cat)) for cat in season_categ]\n",
    "\n",
    "#train_df = train_df.select(exprs+train_df.columns)\n",
    "\n",
    "#train_df =train_df.drop(\"seasoncat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Converted the season column into the string Data type for using the string indexer\n",
    "####String indexer is very useful for encoding the categorical columns with string values as various category labels\n",
    "\n",
    "train_df = train_df.withColumn(\"season\", train_df[\"season\"].cast(StringType()))\n",
    "train_df.printSchema()\n",
    "train_df.show()\n",
    "\n",
    "###Decription of Data after converting to pandas and transposing the description \n",
    "train_df.toPandas().transpose().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training of the various ML Models with Data \n",
    "\n",
    "import pandas as pd\n",
    "from pylab import *\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder , VectorAssembler\n",
    "\n",
    "#String Indexer to create the integer 0,1,2,.... like indexes of the categorical variable \n",
    "stringIndexer = StringIndexer(inputCol = \"season\", outputCol = 'season' + 'Index')\n",
    "\n",
    "#Then One hot encoding --- is called one hot means that if 4 labels then our code contain three feature columns  \n",
    "OHencoder = OneHotEncoder(inputCol=stringIndexer.getOutputCol(), outputCol=\"season_cat\")\n",
    "\n",
    "#Used the pipelines to stremline the whole process\n",
    "stages = []\n",
    "stages += [stringIndexer, OHencoder]\n",
    "\n",
    "## This assembler inputs are the columns which will be converted into the feature vector\n",
    "assemblerInputs = [\"season_cat\"] + ['temp','atemp','humidity','windspeed']\n",
    "\n",
    "### Vector Creator with column features which will be used in the training of the model\n",
    "Vectassembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')\n",
    "\n",
    "### Vector assembler is added to the pipeline\n",
    "stages += [Vectassembler]\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "cols = train_df.columns\n",
    "\n",
    "##Pipleline is created \n",
    "pipeline = Pipeline(stages = stages)\n",
    "\n",
    "### Pipeline fitting with train_df data \n",
    "pipelineModel = pipeline.fit(train_df)\n",
    "\n",
    "## then the whole pipeline model is used to transform the data into the desired way what we want\n",
    "train_df = pipelineModel.transform(train_df)\n",
    "\n",
    "selectedCols = ['features']+cols\n",
    "\n",
    "train_df = train_df.select(selectedCols)\n",
    "\n",
    "\n",
    "### way to look into the Data in the pandas way\n",
    "pd.DataFrame(train_df.take(100), columns=train_df.columns)\n",
    "\n",
    "#train_df.printSchema()\n",
    "#train_df.filter(train_df[\"season\"]== '4').show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Validation Set - set aside training data for validation\n",
    "vtrain_df = train_df.select('features', 'count')\n",
    "splits = vtrain_df.randomSplit([0.7, 0.3])\n",
    "f_train_df = splits[0]\n",
    "f_test_df = splits[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we have imported the linear regression Libraries\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "####### Working on the ML model using Linear regression \n",
    "\n",
    "#Generating the Linear regression model based on the features and other parameters passed\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='count')\n",
    "\n",
    "#Then we fit the training data into the model generated\n",
    "lr_model = lr.fit(f_train_df)\n",
    "\n",
    "pred = lr_model.evaluate(f_test_df)\n",
    "\n",
    "#Show the predicted Grade values along side actual Grade values\n",
    "pred.predictions.show(1000)\n",
    "\n",
    "#Training summary\n",
    "trainingSummary = lr_model.summary\n",
    "\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n",
    "\n",
    "#Then we print the coefficients and the intercept of the linear model we have got after fitting our training data\n",
    "\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Predictions out of the model over the test data\n",
    "\n",
    "lr_predictions = lr_model.transform(f_test_df)\n",
    "lr_predictions.select(\"prediction\",\"count\",\"features\").show(5)\n",
    "\n",
    "#so here we are doing the evaluations over the predictions made on the test data and the corresponding to the actual output\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"count\",metricName=\"r2\")\n",
    "\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us work through the Random Forest Regression\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "\n",
    "featureIndexer =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(train_df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = train_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(labelCol=\"count\",featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer,rf])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"count\",\"indexedFeatures\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(labelCol=\"count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "rfModel = model.stages[0]\n",
    "print(rfModel)  # summary only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us work through the Decision Tree Regression\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(train_df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = train_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor(labelCol=\"count\",featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"count\" , \"features\").show(100)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(labelCol=\"count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "treeModel = model.stages[1]\n",
    "# summary only\n",
    "print(treeModel)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us work through the Gradient Boosted Regression\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(train_df)\n",
    "\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = train_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTRegressor(labelCol=\"count\",featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexer and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "######Saving the model to HDFS for persistence and then using it in our prediction application\n",
    "\n",
    "model.save(\"/user/edureka_960126/models_gbr\")\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"count\" , \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(labelCol=\"count\" , predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GTB Rgeressional Model is looking fine for me as it is working fine \n",
    "# Let us save this model so that it can be pulled from the HDFS when required\n",
    "\n",
    "reloaded_model = PipelineModel.load(\"/user/edureka_960126/models_gbr\")\n",
    "\n",
    "# Make predictions.\n",
    "predictions = reloaded_model.transform(testData)\n",
    "#predictions = reloaded_model.transform(testData.select('features'))\n",
    "\n",
    "#predictions.select(\"prediction\", \"features\").show(5)\n",
    "\n",
    "# Select example rows to display.\n",
    "\n",
    "predictions.select(\"prediction\", \"count\" , \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(labelCol=\"count\" , predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "gbtModel = reloaded_model.stages[1]\n",
    "print(gbtModel)  # summary only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########----------- Commands were used to check the RDBMS connectivity and read/write operations--------------------\n",
    "\n",
    "#Connectivity to the RDBMS for making the application working\n",
    "\n",
    "#Practising the mysql command \n",
    "#dataframe_mysql = spark.read.format(\"jdbc\").option(\"url\", 'jdbc:mysql://dbserver.edu.cloudlab.com/labuser_database').option(\"driver\", \"com.mysql.jdbc.Driver\").option(\"dbtable\", \"emp\").option(\"user\", 'edu_labuser').option(\"password\", \"edureka\").load() \n",
    "\n",
    "#dataframe_mysql.show()\n",
    "\n",
    "#temp_df=train_df.drop(\"features\")\n",
    "\n",
    "#temp_df.write.format('jdbc').options(url='jdbc:mysql://dbserver.edu.cloudlab.com/labuser_database',driver='com.mysql.jdbc.Driver',dbtable='r_table',user='edu_labuser', password='edureka').save()\n",
    "\n",
    "#dataframe_mysql = spark.read.format(\"jdbc\").option(\"url\", 'jdbc:mysql://dbserver.edu.cloudlab.com/labuser_database').option(\"driver\", \"com.mysql.jdbc.Driver\").option(\"dbtable\", \"r_table\").option(\"user\", 'edu_labuser').option(\"password\", \"edureka\").load() \n",
    "\n",
    "#dataframe_mysql.show()\n",
    "\n",
    "### Sample reading of the data from the sample table created in the Database \n",
    "dataframe_mysql = spark.read.format(\"jdbc\").option(\"url\", 'jdbc:mysql://dbserver.edu.cloudlab.com/labuser_database').option(\"driver\", \"com.mysql.jdbc.Driver\").option(\"dbtable\", \"r_table\").option(\"user\", 'edu_labuser').option(\"password\", \"edureka\").load() \n",
    "dataframe_mysql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2 of making an application for reading the file path of the test.csv from the hdfs and then putting the result in the RDBMS\n",
    "\n",
    "#We may get the input from the user for the file location in the hdfs so as to load the CSV but here we are using the test file provided \n",
    "#in the Dataset\n",
    "\n",
    "#Step 1 loading of the file from the hdfs \n",
    "\n",
    "\n",
    "#getting the hdfs file path\n",
    "TEST_CSV = get_hdfs_filepath('test.csv')\n",
    "\n",
    "#loading the csv file from the hdfs file path into the data frame\n",
    "test_df=spark.read.csv(TEST_CSV,inferSchema=True,header=True)\n",
    "\n",
    "\n",
    "#Getting the features vector from the test.csv for inputting into our stored loaded model\n",
    "\n",
    "#This is the string indexer used for encoding the categorical variable column with string categories but\n",
    "#with our schema inference this may not be the case but we are still using this \n",
    "stringIndexer = StringIndexer(inputCol = \"season\", outputCol = 'season' + 'Index')\n",
    "\n",
    "#One hot encoding of the season category \n",
    "OHencoder = OneHotEncoder(inputCol=stringIndexer.getOutputCol(), outputCol=\"season_cat\")\n",
    "\n",
    "#Pipelining of stages\n",
    "stages = []\n",
    "stages += [stringIndexer, OHencoder]\n",
    "\n",
    "#The columns that need to be assembled for getting the features\n",
    "assemblerInputs = [\"season_cat\"] + ['temp','atemp','humidity','windspeed']\n",
    "\n",
    "#Vector Assembler for converting the assembler inputs into features column with vector of all features for putting inside the ML Model\n",
    "Vectassembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')\n",
    "\n",
    "#Now Vector assembler has been added\n",
    "stages += [Vectassembler]\n",
    "\n",
    "\n",
    "###Note the above stages were already created in the training of the model phase but still we have created the above steps to show them again while working with the \n",
    "#Testing Data\n",
    "\n",
    "cols = test_df.columns\n",
    "\n",
    "#Creating the whole pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "\n",
    "#Fitting the testing data to the pipeline of features extraction from the testing data\n",
    "pipelineModel = pipeline.fit(test_df)\n",
    "\n",
    "test_df = pipelineModel.transform(test_df)\n",
    "\n",
    "selectedCols = ['features']+cols\n",
    "\n",
    "test_df = test_df.select(selectedCols)\n",
    "\n",
    "###Now we work on the predictions part using the stored model\n",
    "\n",
    "#reloading the model from hdfs\n",
    "\n",
    "reloaded_model = PipelineModel.load(\"/user/edureka_960126/models_gbr\")\n",
    "\n",
    "# Make predictions.\n",
    "predictions = reloaded_model.transform(test_df)\n",
    "\n",
    "\n",
    "##### Some sample results of error and the sample columns are shown\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\",\"features\").show(5)\n",
    "\n",
    "predictions=predictions.drop(\"features\",\"indexedFeatures\")\n",
    "\n",
    "predictions.show(5)\n",
    "##### Storing the resulting predictions based results into the RDBMS\n",
    "\n",
    "##### Here I am appending the results into the same table but we can take user input and create table according to that\n",
    "predictions.write.mode('append').format('jdbc').options(url='jdbc:mysql://dbserver.edu.cloudlab.com/labuser_database',driver='com.mysql.jdbc.Driver',dbtable='test_predictions',user='edu_labuser', password='edureka').save()\n",
    "\n",
    "\n",
    "#reading of the Data again from the table and show the sample of results\n",
    "dataframe_mysql = spark.read.format(\"jdbc\").option(\"url\", 'jdbc:mysql://dbserver.edu.cloudlab.com/labuser_database').option(\"driver\", \"com.mysql.jdbc.Driver\").option(\"dbtable\", \"test_predictions\").option(\"user\", 'edu_labuser').option(\"password\", \"edureka\").load() \n",
    "dataframe_mysql.show(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:36:15\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:36:30\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:36:45\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:37:00\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:37:15\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:37:30\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:37:45\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:38:00\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:38:15\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:38:30\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:38:45\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:39:00\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:39:15\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:39:30\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:39:45\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:40:00\n",
      "-------------------------------------------\n",
      "1\n",
      "\n",
      "+--------------------+----------------+\n",
      "|            features|      prediction|\n",
      "+--------------------+----------------+\n",
      "|[0.0,0.0,0.0,13.4...|79.7834601142338|\n",
      "+--------------------+----------------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:40:15\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:40:30\n",
      "-------------------------------------------\n",
      "1\n",
      "\n",
      "+--------------------+----------------+\n",
      "|            features|      prediction|\n",
      "+--------------------+----------------+\n",
      "|[0.0,0.0,0.0,13.4...|79.7834601142338|\n",
      "+--------------------+----------------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:40:45\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:41:00\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:41:15\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:41:30\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:41:45\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:42:00\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:42:15\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:42:30\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:42:45\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:43:00\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:43:15\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:43:30\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:43:45\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:44:00\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:44:15\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:44:30\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:44:45\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:45:00\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:45:15\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:45:30\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:45:45\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:46:00\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:46:15\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:46:30\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:46:45\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-07-15 04:47:00\n",
      "-------------------------------------------\n",
      "\n",
      "+--------+----------+\n",
      "|features|prediction|\n",
      "+--------+----------+\n",
      "+--------+----------+\n",
      "\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|season_1|season_2|season_3|weather|temp|atemp|humidity|windspeed|      prediction|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "|     0.0|     0.0|     0.0|      2|13.4| 14.6|    84.0|     18.0|79.7834601142338|\n",
      "+--------+--------+--------+-------+----+-----+--------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Part 3 of the Integration of ML Model for prediction with the Flume \n",
    "\n",
    "#1. Setup flume to push data into spark flume sink.  \n",
    "#2. Configure spark streaming to pull data from spark flume sink using receivers \n",
    "#and predict the demand using model and persist the result to RDBMS. \n",
    "#3. Push messages from flume to test the application. Here application should process and persist the result to RDBMS \n",
    "\n",
    "#Architecture to be followed here\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pyspark.streaming.flume import FlumeUtils\n",
    "\n",
    "import json\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "#These are the variables set for the machine learning model to be applied over the microbatches\n",
    "stages = []\n",
    "\n",
    "assemblerInputs = [\"season_1\",\"season_2\",\"season_3\"] + ['temp','atemp','humidity','windspeed']\n",
    "Vectassembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')\n",
    "stages += [Vectassembler]\n",
    "\n",
    "pipeline = Pipeline(stages = stages)\n",
    "\n",
    "\n",
    "#Model stored in the hdfs to be used for the machine learning application\n",
    "reloaded_model = PipelineModel.load(\"/user/edureka_960126/models_gbr\")\n",
    "\n",
    "\n",
    "#Seasons code for converting each season into code\n",
    "seasons_code={1 : [0.0,0.0,0.0] , 2 : [0.0,1.0,0.0], 3:[0.0,0.0,1.0], 4:[1.0,0.0,0.0]}\n",
    "\n",
    "#Schema is being set for the dataframe creation out of the records\n",
    "cSchema = StructType([StructField(\"season_1\", DoubleType(),nullable=True),StructField(\"season_2\", DoubleType(),nullable=True),StructField(\"season_3\", DoubleType(),nullable=True),StructField(\"weather\", IntegerType(),nullable=True),\n",
    "                    \n",
    "        StructField(\"temp\", DoubleType(),nullable=True),StructField(\"atemp\", DoubleType(),nullable=True),StructField(\"humidity\", DoubleType(),nullable=True)\n",
    "                     \n",
    "                     ,StructField(\"windspeed\", DoubleType(),nullable=True)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Process for working out the prediction\n",
    "def process(rdd):\n",
    "    \n",
    "        # Get the singleton instance of SparkSession\n",
    "        #spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "\n",
    "        #Conversion to the Data Frame to work on the dataframe for working out the predictions\n",
    "        df = spark.createDataFrame(rdd,schema=cSchema)\n",
    "\n",
    "        #Now we work on the dataframe for producing the predictions\n",
    "        cols = df.columns\n",
    "\n",
    "        #Getting the output after passing the data frame through vector assembler to produce the dataframe with vectorised features\n",
    "        pipelineModel = pipeline.fit(df)\n",
    "        df = pipelineModel.transform(df)\n",
    "\n",
    "        #dataframe features selection \n",
    "        selectedCols = ['features']+cols\n",
    "        df = df.select(selectedCols)\n",
    "\n",
    "        # Make predictions.\n",
    "        temp_df=reloaded_model.transform(df)\n",
    "        \n",
    "        temp_df.select(\"features\",\"prediction\").show()\n",
    "        \n",
    "        temp_df=temp_df.drop(\"features\",\"indexedFeatures\")\n",
    "        \n",
    "        #Storing the Data in the table created for storing the results of the predictions into the RDBMS Table       \n",
    "        temp_df.write.mode('append').format('jdbc').options(url='jdbc:mysql://dbserver.edu.cloudlab.com/labuser_database',driver='com.mysql.jdbc.Driver',dbtable='streaming_result',user='edu_labuser', password='edureka').save()\n",
    "        \n",
    "        #reading of the Data again from the table and show the sample of results\n",
    "        dataframe_mysql = spark.read.format(\"jdbc\").option(\"url\", 'jdbc:mysql://dbserver.edu.cloudlab.com/labuser_database').option(\"driver\", \"com.mysql.jdbc.Driver\").option(\"dbtable\", \"streaming_result\").option(\"user\", 'edu_labuser').option(\"password\", \"edureka\").load() \n",
    "        dataframe_mysql.show(5)\n",
    "        \n",
    "\n",
    "ssc= StreamingContext(spark.sparkContext, 15)\n",
    "\n",
    "#Flume stream is generated after the spark streaming receiver is connected to custom spark streaming sink created at the host with a given port\n",
    "flumeStream = FlumeUtils.createPollingStream(ssc, [('ip-20-0-41-164.ec2.internal' , 9090)])\n",
    "\n",
    "#Flume_microbatches count\n",
    "flumeStream.count().pprint()\n",
    "\n",
    "#flumeStream.foreachRDD(lambda rdd: rdd.foreach(sendRecord))  \n",
    " \n",
    "#RDD[Strings]\n",
    "\n",
    "#Here we get the strings of the json format input data \n",
    "lines = flumeStream.map(lambda x: x[1])\n",
    "\n",
    "#RDD of Dicts or JSON objects by extracting the json objects from the string \n",
    "records_dict=lines.map(lambda x: json.loads(x))\n",
    "\n",
    "#Rows RDD rows rdd is created here\n",
    "rows_rdd=records_dict.map(lambda res: Row(seasons_code[res['season']][0],seasons_code[res['season']][1],seasons_code[res['season']][2],res['weather'],res['temp'],res['atemp'],res['humidity'],res['windspeed'] ))\n",
    "\n",
    "rows_rdd.foreachRDD(process)\n",
    "\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Sample code created By me for checking the working over the input json data which will be streamed to spark streaming through \n",
    "#### Flume before I finalised the Part 3 \n",
    "\n",
    "#Dataset that will be coming in the each push of message at the flume source\n",
    "import json\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "seasons_code={1 : [0.0,0.0,0.0] , 2 : [0.0,1.0,0.0], 3:[0.0,0.0,1.0], 4:[1.0,0.0,0.0]}\n",
    "\n",
    "reloaded_model = PipelineModel.load(\"/user/edureka_960126/model\")\n",
    "\n",
    "#Sample input request from http source through Flume in microbatches\n",
    "\n",
    "x='{\"season\" : 1 , \"weather\" :2 , \"temp\" : 13.4 , \"atemp\" : 14.6 , \"humidity\" : 84.0 , \"windspeed\" : 18.0}'\n",
    "\n",
    "res = json.loads(x)\n",
    "\n",
    "features=seasons_code[res['season']]+[res['weather'],res['temp'],res['atemp'],res['humidity'],res['windspeed'] ]\n",
    "\n",
    "r = Row(features)\n",
    "\n",
    "cSchema = StructType([StructField(\"season_1\", DoubleType(),nullable=True),StructField(\"season_2\", DoubleType(),nullable=True),StructField(\"season_3\", DoubleType(),nullable=True),StructField(\"weather\", IntegerType(),nullable=True),\n",
    "                    \n",
    "        StructField(\"temp\", DoubleType(),nullable=True),StructField(\"atemp\", DoubleType(),nullable=True),StructField(\"humidity\", DoubleType(),nullable=True)\n",
    "                     \n",
    "                     ,StructField(\"windspeed\", DoubleType(),nullable=True)])\n",
    "\n",
    "df=spark.createDataFrame(r,schema=cSchema)\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n",
    "#stringIndexer = StringIndexer(inputCol = \"season\", outputCol = 'season' + 'Index')\n",
    "\n",
    "#OHencoder = OneHotEncoder(inputCol=stringIndexer.getOutputCol(), outputCol=\"season_cat\")\n",
    "\n",
    "stages = []\n",
    "\n",
    "#stages += [stringIndexer, OHencoder]\n",
    "\n",
    "assemblerInputs = [\"season_1\",\"season_2\",\"season_3\"] + ['temp','atemp','humidity','windspeed']\n",
    "\n",
    "Vectassembler = VectorAssembler(inputCols=assemblerInputs, outputCol='features')\n",
    "\n",
    "stages += [Vectassembler]\n",
    "\n",
    "cols = df.columns\n",
    "\n",
    "pipeline = Pipeline(stages = stages)\n",
    "\n",
    "pipelineModel = pipeline.fit(df)\n",
    "\n",
    "df = pipelineModel.transform(df)\n",
    "\n",
    "selectedCols = ['features']+cols\n",
    "\n",
    "df = df.select(selectedCols)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions.\n",
    "reloaded_model.transform(df).select(\"features\",\"prediction\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
